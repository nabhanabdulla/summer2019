{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward prop for a 3 layer NN"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"..\\Screenshots\\Screenshot (712).png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(3, 1) # random input vector of three numbers (3, 1)\n",
    "\n",
    "W1 = 2 * np.random.random((4, 3)) - 1 #(4, 3)\n",
    "W2 = 2 * np.random.random((4, 4)) - 1 #(4, 3)\n",
    "W3 = 2 * np.random.random((1, 4)) - 1 #(1, 4)\n",
    "\n",
    "b1 = np.random.random((4, 1))\n",
    "b2 = np.random.random((4, 1))\n",
    "b3 = np.random.random((1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.10223226]]\n"
     ]
    }
   ],
   "source": [
    "f = lambda x: 1.0/(1.0 + np.exp(-x)) # computes sigmoid activation\n",
    "\n",
    "'''\n",
    "    Every neuron in hidden layer 1 computes w1 * x, ie (1, 3) * (3, 1) => (1, 1), passes this through \n",
    "    sigmoid activation. As the hidden layer 1 has 4 neurons, the weight matrix for the layer, W1 is 4 * (1, 3) => (4, 3)\n",
    "    and the output is 4 * (1, 1) => (4, 1)\n",
    "    \n",
    "    Each neuron will have a bias value and each edge carries a weight. As bias is the property of a neuron, size of bias \n",
    "    is (no.of neurons, 1)\n",
    "'''\n",
    "h1 = f(np.dot(W1, x) + b1) # calculate first hidden layer activation (4, 1)\n",
    "'''\n",
    "    Every neuron in hidden layer 2 computes w1 * h1, ie (1, 4) * (4, 1) => (1, 1), passes this through \n",
    "    sigmoid activation. As the hidden layer 2 has 4 neurons, the weight matrix for the layer, W2 is 4 * (1, 4) => (4, 4)\n",
    "    and the output is 4 * (1, 1) => (4, 1)\n",
    "'''\n",
    "h2 = f(np.dot(W2, h1) + b2) # calculate second hidden layer activation (4, 1)\n",
    "\n",
    "'''\n",
    "    The single neuron in the output layer computes w3 * h2, ie (1, 4) * (4, 1) => (1, 1).\n",
    "'''\n",
    "out = np.dot(W3, h2) + b3\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backprop for a 2 layer NN with sigmoid activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading: [A Neural Network in 11 lines of Python](https://iamtrask.github.io/2015/07/12/basic-python-network/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input - 3 neurons\n",
    "Hidden layer 1 - 4 nuerons\n",
    "Ouput - 1 neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 - MSE: 0.2521887279676235\n",
      "Iteration 20000 - MSE: 4.62903354203041e-05\n",
      "Iteration 40000 - MSE: 2.209679630566262e-05\n",
      "Predicted: \n",
      " [[0.0040161  0.99558957 0.99692832 0.00356169]], \n",
      " Actual: \n",
      " [[0 1 1 0]]\n"
     ]
    }
   ],
   "source": [
    "# seed random numbers to make calculation\n",
    "# deterministic (just a good practice)\n",
    "np.random.seed(1)\n",
    "\n",
    "# input dataset\n",
    "X = np.array([ [0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]]).T #(3, 4)\n",
    "# output dataset           \n",
    "y = np.array([[0, 1, 1, 0]])  #(1, 4)\n",
    "\n",
    "# initialize weights randomly with mean 0\n",
    "W1 = 2 * np.random.random((4, 3)) - 1 #(4, 3)\n",
    "W2 = 2 * np.random.random((1, 4)) - 1 #(1, 4)\n",
    "\n",
    "# input layer\n",
    "for j in range(60000):\n",
    "    # forward prop for first hidden layer\n",
    "    l1 = 1/(1 + np.exp(- (np.dot(W1, X)) )) #(4, 4)\n",
    "    # forward prop for second hidden layer\n",
    "    l2 = 1/(1 + np.exp(- (np.dot(W2, l1)) ))#(1, 4)\n",
    "\n",
    "    \n",
    "    # gradient from its forward node\n",
    "    l2_forward_grad = l2 - y # error - amount by which the NN missed\n",
    "    \n",
    "    # local gradient\n",
    "    l2_local_grad = l2 * (1 - l2) \n",
    "    \n",
    "    # Gradient of output layer\n",
    "    l2_delta = l2_forward_grad * l2_local_grad #(1, 4) This is the error of the network scaled by the confidence\n",
    "    \n",
    "    \n",
    "     # how much did each l1 value contribute to the l2 error (according to the weights)?\n",
    "    # Weighting l2_delta by the weights in W2, we can calculate the error in the middle/hidden layer.\n",
    "    l1_forward_grad = W2.T.dot(l2_delta)\n",
    "    # local gradient\n",
    "    l1_local_grad = l1 * (1 - l1)\n",
    "    # gradient of hidden layer 1\n",
    "    l1_delta = l1_forward_grad * l1_local_grad #(4, 4)\n",
    "    \n",
    "    W1 -= l1_delta.dot(X.T) #(4, 3)\n",
    "    W2 -= l2_delta.dot(l1.T) #(4, 4)\n",
    "\n",
    "    if j%20000 == 0:\n",
    "        error = np.mean( pow(abs(l2 - y), 2) )\n",
    "        print(\"Iteration {} - MSE: {}\".format(j, error))\n",
    "\n",
    "print(\"Predicted: \\n {}, \\n Actual: \\n {}\".format(l2, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
