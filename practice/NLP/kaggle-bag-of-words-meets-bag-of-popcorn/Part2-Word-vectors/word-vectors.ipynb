{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.28.2\n"
     ]
    }
   ],
   "source": [
    "import cython\n",
    "print(cython.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000 25000 50000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "train = pd.read_csv(\"../Part1-BOW/labeledTrainData.tsv\", header=0, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "test = pd.read_csv(\"../Part1-BOW/testData.tsv\", header=0, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "unlabeled_train = pd.read_csv(\"unlabeledTrainData.tsv\", header=0, delimiter='\\t', quoting=csv.QUOTE_NONE)\n",
    "\n",
    "print(train['review'].size, test['review'].size, unlabeled_train['review'].size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bsoup\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "def review_to_wordlist(review, remove_stopwords=False):\n",
    "    review_text = bsoup(review, 'html.parser').get_text()\n",
    "    review_text = re.sub(\"[^a-zA-Z]\", \" \", review_text)\n",
    "    words = review_text.lower().split()\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [word for word in words if word not in stops]\n",
    "        \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# for sentence tokenization\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_to_sentences(review, tokenizer, remove_stopwords=False):\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    \n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(review_to_wordlist(raw_sentence, remove_stopwords))\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▊                                                                                            | 205/25000 [00:00<00:52, 468.65it/s]C:\\Users\\nabhu\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "  6%|█████▋                                                                                      | 1544/25000 [00:03<00:51, 454.32it/s]C:\\Users\\nabhu\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'...'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      "C:\\Users\\nabhu\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.happierabroad.com\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 25000/25000 [00:57<00:00, 433.28it/s]\n",
      "  1%|▌                                                                                            | 302/50000 [00:00<02:17, 362.34it/s]C:\\Users\\nabhu\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.archive.org/details/LovefromaStranger\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "  5%|████▋                                                                                       | 2537/50000 [00:05<01:34, 501.42it/s]C:\\Users\\nabhu\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.loosechangeguide.com/LooseChangeGuide.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      " 14%|█████████████▏                                                                              | 7150/50000 [00:16<01:28, 482.18it/s]C:\\Users\\nabhu\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'... ...'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      " 35%|███████████████████████████████▊                                                           | 17478/50000 [00:40<01:14, 438.72it/s]C:\\Users\\nabhu\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'....'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      " 43%|██████████████████████████████████████▉                                                    | 21396/50000 [00:49<01:03, 453.90it/s]C:\\Users\\nabhu\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.msnbc.msn.com/id/4972055/site/newsweek/\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      " 62%|████████████████████████████████████████████████████████▊                                  | 31182/50000 [01:09<00:40, 459.82it/s]C:\\Users\\nabhu\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      " 72%|█████████████████████████████████████████████████████████████████▍                         | 35971/50000 [01:20<00:29, 481.52it/s]C:\\Users\\nabhu\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://www.youtube.com/watch?v=a0KSqelmgN8\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      " 88%|████████████████████████████████████████████████████████████████████████████████▏          | 44046/50000 [01:38<00:14, 407.42it/s]C:\\Users\\nabhu\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:219: UserWarning: \"b'.. .'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  ' Beautiful Soup.' % markup)\n",
      " 97%|████████████████████████████████████████████████████████████████████████████████████████▏  | 48421/50000 [01:48<00:04, 377.29it/s]C:\\Users\\nabhu\\Anaconda3\\lib\\site-packages\\bs4\\__init__.py:282: UserWarning: \"http://jake-weird.blogspot.com/2007/08/beneath.html\"\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  ' that document to Beautiful Soup.' % decoded_markup\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████| 50000/50000 [01:52<00:00, 446.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# to see the progress of the loop\n",
    "from tqdm import tqdm\n",
    "\n",
    "sentences = []\n",
    "\n",
    "for review in tqdm(train[\"review\"]):\n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "for review in tqdm(unlabeled_train[\"review\"]):\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "795538\n",
      "['it', 'does', 'try', 'and', 'give', 'off', 'a', 'wholesome', 'message', 'and', 'ironically', 'mj', 's', 'bestest', 'buddy', 'in', 'this', 'movie', 'is', 'a', 'girl']\n"
     ]
    }
   ],
   "source": [
    "print(len(sentences))\n",
    "print(sentences[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 100    # Word vector dimensionality                      \n",
    "min_word_count = 40   # Minimum word count                        \n",
    "num_workers = 4       # Number of threads to run in parallel\n",
    "context = 10          # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nabhu\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "2019-05-06 11:33:09,255 : INFO : 'pattern' package not found; tag filters are not available for English\n",
      "2019-05-06 11:33:09,325 : INFO : collecting all words and their counts\n",
      "2019-05-06 11:33:09,341 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-05-06 11:33:09,425 : INFO : PROGRESS: at sentence #10000, processed 225803 words, keeping 17776 word types\n",
      "2019-05-06 11:33:09,493 : INFO : PROGRESS: at sentence #20000, processed 451892 words, keeping 24948 word types\n",
      "2019-05-06 11:33:09,576 : INFO : PROGRESS: at sentence #30000, processed 671314 words, keeping 30034 word types\n",
      "2019-05-06 11:33:09,645 : INFO : PROGRESS: at sentence #40000, processed 897814 words, keeping 34348 word types\n",
      "2019-05-06 11:33:09,744 : INFO : PROGRESS: at sentence #50000, processed 1116962 words, keeping 37761 word types\n",
      "2019-05-06 11:33:09,813 : INFO : PROGRESS: at sentence #60000, processed 1338403 words, keeping 40723 word types\n",
      "2019-05-06 11:33:09,894 : INFO : PROGRESS: at sentence #70000, processed 1561579 words, keeping 43333 word types\n",
      "2019-05-06 11:33:10,028 : INFO : PROGRESS: at sentence #80000, processed 1780886 words, keeping 45714 word types\n",
      "2019-05-06 11:33:10,120 : INFO : PROGRESS: at sentence #90000, processed 2004995 words, keeping 48135 word types\n",
      "2019-05-06 11:33:10,216 : INFO : PROGRESS: at sentence #100000, processed 2226966 words, keeping 50207 word types\n",
      "2019-05-06 11:33:10,296 : INFO : PROGRESS: at sentence #110000, processed 2446580 words, keeping 52081 word types\n",
      "2019-05-06 11:33:10,376 : INFO : PROGRESS: at sentence #120000, processed 2668775 words, keeping 54119 word types\n",
      "2019-05-06 11:33:10,467 : INFO : PROGRESS: at sentence #130000, processed 2894303 words, keeping 55847 word types\n",
      "2019-05-06 11:33:10,783 : INFO : PROGRESS: at sentence #140000, processed 3107005 words, keeping 57346 word types\n",
      "2019-05-06 11:33:11,063 : INFO : PROGRESS: at sentence #150000, processed 3332627 words, keeping 59055 word types\n",
      "2019-05-06 11:33:11,315 : INFO : PROGRESS: at sentence #160000, processed 3555315 words, keeping 60617 word types\n",
      "2019-05-06 11:33:11,490 : INFO : PROGRESS: at sentence #170000, processed 3778655 words, keeping 62077 word types\n",
      "2019-05-06 11:33:11,595 : INFO : PROGRESS: at sentence #180000, processed 3999236 words, keeping 63496 word types\n",
      "2019-05-06 11:33:11,690 : INFO : PROGRESS: at sentence #190000, processed 4224449 words, keeping 64794 word types\n",
      "2019-05-06 11:33:11,810 : INFO : PROGRESS: at sentence #200000, processed 4448603 words, keeping 66087 word types\n",
      "2019-05-06 11:33:11,916 : INFO : PROGRESS: at sentence #210000, processed 4669967 words, keeping 67390 word types\n",
      "2019-05-06 11:33:12,039 : INFO : PROGRESS: at sentence #220000, processed 4894968 words, keeping 68697 word types\n",
      "2019-05-06 11:33:12,112 : INFO : PROGRESS: at sentence #230000, processed 5117545 words, keeping 69958 word types\n",
      "2019-05-06 11:33:12,193 : INFO : PROGRESS: at sentence #240000, processed 5345050 words, keeping 71167 word types\n",
      "2019-05-06 11:33:12,275 : INFO : PROGRESS: at sentence #250000, processed 5559165 words, keeping 72351 word types\n",
      "2019-05-06 11:33:12,378 : INFO : PROGRESS: at sentence #260000, processed 5779146 words, keeping 73478 word types\n",
      "2019-05-06 11:33:12,461 : INFO : PROGRESS: at sentence #270000, processed 6000435 words, keeping 74767 word types\n",
      "2019-05-06 11:33:12,560 : INFO : PROGRESS: at sentence #280000, processed 6226314 words, keeping 76369 word types\n",
      "2019-05-06 11:33:12,659 : INFO : PROGRESS: at sentence #290000, processed 6449474 words, keeping 77839 word types\n",
      "2019-05-06 11:33:12,759 : INFO : PROGRESS: at sentence #300000, processed 6674077 words, keeping 79171 word types\n",
      "2019-05-06 11:33:12,844 : INFO : PROGRESS: at sentence #310000, processed 6899391 words, keeping 80480 word types\n",
      "2019-05-06 11:33:12,942 : INFO : PROGRESS: at sentence #320000, processed 7124278 words, keeping 81808 word types\n",
      "2019-05-06 11:33:13,028 : INFO : PROGRESS: at sentence #330000, processed 7346021 words, keeping 83030 word types\n",
      "2019-05-06 11:33:13,126 : INFO : PROGRESS: at sentence #340000, processed 7575533 words, keeping 84280 word types\n",
      "2019-05-06 11:33:13,224 : INFO : PROGRESS: at sentence #350000, processed 7798803 words, keeping 85425 word types\n",
      "2019-05-06 11:33:13,372 : INFO : PROGRESS: at sentence #360000, processed 8019466 words, keeping 86596 word types\n",
      "2019-05-06 11:33:13,508 : INFO : PROGRESS: at sentence #370000, processed 8246654 words, keeping 87708 word types\n",
      "2019-05-06 11:33:13,624 : INFO : PROGRESS: at sentence #380000, processed 8471801 words, keeping 88878 word types\n",
      "2019-05-06 11:33:13,743 : INFO : PROGRESS: at sentence #390000, processed 8701551 words, keeping 89907 word types\n",
      "2019-05-06 11:33:13,858 : INFO : PROGRESS: at sentence #400000, processed 8924500 words, keeping 90916 word types\n",
      "2019-05-06 11:33:13,974 : INFO : PROGRESS: at sentence #410000, processed 9145850 words, keeping 91880 word types\n",
      "2019-05-06 11:33:14,108 : INFO : PROGRESS: at sentence #420000, processed 9366930 words, keeping 92912 word types\n",
      "2019-05-06 11:33:14,224 : INFO : PROGRESS: at sentence #430000, processed 9594467 words, keeping 93932 word types\n",
      "2019-05-06 11:33:14,324 : INFO : PROGRESS: at sentence #440000, processed 9821218 words, keeping 94906 word types\n",
      "2019-05-06 11:33:14,457 : INFO : PROGRESS: at sentence #450000, processed 10044980 words, keeping 96036 word types\n",
      "2019-05-06 11:33:14,608 : INFO : PROGRESS: at sentence #460000, processed 10277740 words, keeping 97088 word types\n",
      "2019-05-06 11:33:14,727 : INFO : PROGRESS: at sentence #470000, processed 10505665 words, keeping 97933 word types\n",
      "2019-05-06 11:33:14,825 : INFO : PROGRESS: at sentence #480000, processed 10726049 words, keeping 98862 word types\n",
      "2019-05-06 11:33:14,976 : INFO : PROGRESS: at sentence #490000, processed 10952793 words, keeping 99871 word types\n",
      "2019-05-06 11:33:15,076 : INFO : PROGRESS: at sentence #500000, processed 11174449 words, keeping 100765 word types\n",
      "2019-05-06 11:33:15,158 : INFO : PROGRESS: at sentence #510000, processed 11399724 words, keeping 101699 word types\n",
      "2019-05-06 11:33:15,241 : INFO : PROGRESS: at sentence #520000, processed 11623075 words, keeping 102598 word types\n",
      "2019-05-06 11:33:15,311 : INFO : PROGRESS: at sentence #530000, processed 11847473 words, keeping 103400 word types\n",
      "2019-05-06 11:33:15,410 : INFO : PROGRESS: at sentence #540000, processed 12072088 words, keeping 104265 word types\n",
      "2019-05-06 11:33:15,493 : INFO : PROGRESS: at sentence #550000, processed 12297639 words, keeping 105133 word types\n",
      "2019-05-06 11:33:15,575 : INFO : PROGRESS: at sentence #560000, processed 12518929 words, keeping 105997 word types\n",
      "2019-05-06 11:33:15,644 : INFO : PROGRESS: at sentence #570000, processed 12748076 words, keeping 106787 word types\n",
      "2019-05-06 11:33:15,745 : INFO : PROGRESS: at sentence #580000, processed 12969572 words, keeping 107665 word types\n",
      "2019-05-06 11:33:15,811 : INFO : PROGRESS: at sentence #590000, processed 13195097 words, keeping 108501 word types\n",
      "2019-05-06 11:33:15,907 : INFO : PROGRESS: at sentence #600000, processed 13417295 words, keeping 109218 word types\n",
      "2019-05-06 11:33:15,978 : INFO : PROGRESS: at sentence #610000, processed 13638318 words, keeping 110092 word types\n",
      "2019-05-06 11:33:16,061 : INFO : PROGRESS: at sentence #620000, processed 13864643 words, keeping 110837 word types\n",
      "2019-05-06 11:33:16,129 : INFO : PROGRESS: at sentence #630000, processed 14088929 words, keeping 111610 word types\n",
      "2019-05-06 11:33:16,208 : INFO : PROGRESS: at sentence #640000, processed 14309712 words, keeping 112416 word types\n",
      "2019-05-06 11:33:16,291 : INFO : PROGRESS: at sentence #650000, processed 14535468 words, keeping 113196 word types\n",
      "2019-05-06 11:33:16,362 : INFO : PROGRESS: at sentence #660000, processed 14758258 words, keeping 113945 word types\n",
      "2019-05-06 11:33:16,428 : INFO : PROGRESS: at sentence #670000, processed 14981651 words, keeping 114643 word types\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-06 11:33:16,541 : INFO : PROGRESS: at sentence #680000, processed 15206483 words, keeping 115354 word types\n",
      "2019-05-06 11:33:16,644 : INFO : PROGRESS: at sentence #690000, processed 15428676 words, keeping 116131 word types\n",
      "2019-05-06 11:33:16,740 : INFO : PROGRESS: at sentence #700000, processed 15657382 words, keeping 116943 word types\n",
      "2019-05-06 11:33:16,826 : INFO : PROGRESS: at sentence #710000, processed 15880371 words, keeping 117596 word types\n",
      "2019-05-06 11:33:16,914 : INFO : PROGRESS: at sentence #720000, processed 16105658 words, keeping 118221 word types\n",
      "2019-05-06 11:33:17,008 : INFO : PROGRESS: at sentence #730000, processed 16332039 words, keeping 118954 word types\n",
      "2019-05-06 11:33:17,106 : INFO : PROGRESS: at sentence #740000, processed 16553072 words, keeping 119668 word types\n",
      "2019-05-06 11:33:17,193 : INFO : PROGRESS: at sentence #750000, processed 16771399 words, keeping 120295 word types\n",
      "2019-05-06 11:33:17,260 : INFO : PROGRESS: at sentence #760000, processed 16990803 words, keeping 120930 word types\n",
      "2019-05-06 11:33:17,328 : INFO : PROGRESS: at sentence #770000, processed 17217940 words, keeping 121703 word types\n",
      "2019-05-06 11:33:17,409 : INFO : PROGRESS: at sentence #780000, processed 17448086 words, keeping 122402 word types\n",
      "2019-05-06 11:33:17,478 : INFO : PROGRESS: at sentence #790000, processed 17675162 words, keeping 123066 word types\n",
      "2019-05-06 11:33:17,535 : INFO : collected 123504 word types from a corpus of 17798263 raw words and 795538 sentences\n",
      "2019-05-06 11:33:17,535 : INFO : Loading a fresh vocabulary\n",
      "2019-05-06 11:33:17,626 : INFO : min_count=40 retains 16490 unique words (13% of original 123504, drops 107014)\n",
      "2019-05-06 11:33:17,626 : INFO : min_count=40 leaves 17239118 word corpus (96% of original 17798263, drops 559145)\n",
      "2019-05-06 11:33:17,727 : INFO : deleting the raw counts dictionary of 123504 items\n",
      "2019-05-06 11:33:17,735 : INFO : sample=0.001 downsamples 48 most-common words\n",
      "2019-05-06 11:33:17,739 : INFO : downsampling leaves estimated 12749794 word corpus (74.0% of prior 17239118)\n",
      "2019-05-06 11:33:17,874 : INFO : estimated required memory for 16490 words and 100 dimensions: 21437000 bytes\n",
      "2019-05-06 11:33:17,874 : INFO : resetting layer weights\n",
      "2019-05-06 11:33:18,210 : INFO : training model with 4 workers on 16490 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2019-05-06 11:33:19,261 : INFO : EPOCH 1 - PROGRESS: at 4.59% examples, 578831 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:33:20,268 : INFO : EPOCH 1 - PROGRESS: at 9.40% examples, 592374 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:33:21,277 : INFO : EPOCH 1 - PROGRESS: at 13.71% examples, 572412 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:33:22,320 : INFO : EPOCH 1 - PROGRESS: at 18.42% examples, 573438 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:33:23,338 : INFO : EPOCH 1 - PROGRESS: at 22.82% examples, 568432 words/s, in_qsize 8, out_qsize 1\n",
      "2019-05-06 11:33:24,343 : INFO : EPOCH 1 - PROGRESS: at 27.14% examples, 564699 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:33:25,349 : INFO : EPOCH 1 - PROGRESS: at 30.74% examples, 549257 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:33:26,352 : INFO : EPOCH 1 - PROGRESS: at 35.61% examples, 556874 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:33:27,359 : INFO : EPOCH 1 - PROGRESS: at 40.80% examples, 568510 words/s, in_qsize 6, out_qsize 1\n",
      "2019-05-06 11:33:28,369 : INFO : EPOCH 1 - PROGRESS: at 46.56% examples, 584436 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:33:29,362 : INFO : EPOCH 1 - PROGRESS: at 52.21% examples, 596613 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:33:30,364 : INFO : EPOCH 1 - PROGRESS: at 57.52% examples, 603935 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:33:31,381 : INFO : EPOCH 1 - PROGRESS: at 63.16% examples, 612667 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:33:32,387 : INFO : EPOCH 1 - PROGRESS: at 68.88% examples, 620701 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:33:33,382 : INFO : EPOCH 1 - PROGRESS: at 74.80% examples, 629724 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:33:34,389 : INFO : EPOCH 1 - PROGRESS: at 80.87% examples, 638149 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:33:35,397 : INFO : EPOCH 1 - PROGRESS: at 86.02% examples, 638980 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:33:36,399 : INFO : EPOCH 1 - PROGRESS: at 91.98% examples, 645783 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:33:37,418 : INFO : EPOCH 1 - PROGRESS: at 97.78% examples, 650331 words/s, in_qsize 8, out_qsize 0\n",
      "2019-05-06 11:33:37,780 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-06 11:33:37,805 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-06 11:33:37,817 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-06 11:33:37,817 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-06 11:33:37,821 : INFO : EPOCH - 1 : training on 17798263 raw words (12749012 effective words) took 19.6s, 651522 effective words/s\n",
      "2019-05-06 11:33:38,839 : INFO : EPOCH 2 - PROGRESS: at 4.27% examples, 537039 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:33:39,861 : INFO : EPOCH 2 - PROGRESS: at 9.87% examples, 615058 words/s, in_qsize 8, out_qsize 0\n",
      "2019-05-06 11:33:40,862 : INFO : EPOCH 2 - PROGRESS: at 15.23% examples, 636267 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:33:41,866 : INFO : EPOCH 2 - PROGRESS: at 21.18% examples, 663794 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:33:42,866 : INFO : EPOCH 2 - PROGRESS: at 27.03% examples, 679406 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:33:43,869 : INFO : EPOCH 2 - PROGRESS: at 33.03% examples, 692238 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:33:44,874 : INFO : EPOCH 2 - PROGRESS: at 38.45% examples, 691574 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:33:45,883 : INFO : EPOCH 2 - PROGRESS: at 43.69% examples, 688444 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:33:46,892 : INFO : EPOCH 2 - PROGRESS: at 49.15% examples, 689332 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:33:47,913 : INFO : EPOCH 2 - PROGRESS: at 53.99% examples, 681275 words/s, in_qsize 6, out_qsize 1\n",
      "2019-05-06 11:33:48,938 : INFO : EPOCH 2 - PROGRESS: at 58.95% examples, 676268 words/s, in_qsize 8, out_qsize 0\n",
      "2019-05-06 11:33:49,946 : INFO : EPOCH 2 - PROGRESS: at 64.56% examples, 678779 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:33:50,946 : INFO : EPOCH 2 - PROGRESS: at 70.60% examples, 685747 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:33:51,952 : INFO : EPOCH 2 - PROGRESS: at 76.60% examples, 691191 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:33:53,004 : INFO : EPOCH 2 - PROGRESS: at 81.87% examples, 687715 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:33:53,995 : INFO : EPOCH 2 - PROGRESS: at 85.74% examples, 675607 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:33:55,005 : INFO : EPOCH 2 - PROGRESS: at 91.60% examples, 679902 words/s, in_qsize 8, out_qsize 0\n",
      "2019-05-06 11:33:56,007 : INFO : EPOCH 2 - PROGRESS: at 97.67% examples, 684728 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:33:56,453 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-06 11:33:56,461 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-06 11:33:56,469 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-06 11:33:56,481 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-06 11:33:56,485 : INFO : EPOCH - 2 : training on 17798263 raw words (12749550 effective words) took 18.7s, 683606 effective words/s\n",
      "2019-05-06 11:33:57,491 : INFO : EPOCH 3 - PROGRESS: at 4.75% examples, 607695 words/s, in_qsize 8, out_qsize 0\n",
      "2019-05-06 11:33:58,512 : INFO : EPOCH 3 - PROGRESS: at 7.30% examples, 460599 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:33:59,538 : INFO : EPOCH 3 - PROGRESS: at 10.20% examples, 425341 words/s, in_qsize 8, out_qsize 0\n",
      "2019-05-06 11:34:00,565 : INFO : EPOCH 3 - PROGRESS: at 14.39% examples, 448383 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:34:01,571 : INFO : EPOCH 3 - PROGRESS: at 19.26% examples, 480363 words/s, in_qsize 8, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-06 11:34:02,574 : INFO : EPOCH 3 - PROGRESS: at 23.98% examples, 500142 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:34:03,585 : INFO : EPOCH 3 - PROGRESS: at 28.70% examples, 513497 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:34:04,587 : INFO : EPOCH 3 - PROGRESS: at 33.72% examples, 527574 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:34:05,594 : INFO : EPOCH 3 - PROGRESS: at 38.51% examples, 536733 words/s, in_qsize 8, out_qsize 0\n",
      "2019-05-06 11:34:06,605 : INFO : EPOCH 3 - PROGRESS: at 41.76% examples, 524257 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:34:07,603 : INFO : EPOCH 3 - PROGRESS: at 44.74% examples, 511172 words/s, in_qsize 6, out_qsize 1\n",
      "2019-05-06 11:34:08,606 : INFO : EPOCH 3 - PROGRESS: at 47.95% examples, 503080 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:34:09,609 : INFO : EPOCH 3 - PROGRESS: at 53.43% examples, 518117 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:34:10,639 : INFO : EPOCH 3 - PROGRESS: at 57.85% examples, 520742 words/s, in_qsize 8, out_qsize 0\n",
      "2019-05-06 11:34:11,642 : INFO : EPOCH 3 - PROGRESS: at 61.36% examples, 516047 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:34:12,657 : INFO : EPOCH 3 - PROGRESS: at 64.79% examples, 510679 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:34:13,659 : INFO : EPOCH 3 - PROGRESS: at 68.66% examples, 509540 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:34:14,674 : INFO : EPOCH 3 - PROGRESS: at 71.99% examples, 504775 words/s, in_qsize 8, out_qsize 0\n",
      "2019-05-06 11:34:15,689 : INFO : EPOCH 3 - PROGRESS: at 76.82% examples, 509876 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:34:16,696 : INFO : EPOCH 3 - PROGRESS: at 82.50% examples, 520318 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:34:17,709 : INFO : EPOCH 3 - PROGRESS: at 86.19% examples, 517779 words/s, in_qsize 8, out_qsize 0\n",
      "2019-05-06 11:34:18,715 : INFO : EPOCH 3 - PROGRESS: at 91.43% examples, 524672 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:34:19,711 : INFO : EPOCH 3 - PROGRESS: at 96.86% examples, 531491 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:34:20,371 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-06 11:34:20,387 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-06 11:34:20,391 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-06 11:34:20,403 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-06 11:34:20,407 : INFO : EPOCH - 3 : training on 17798263 raw words (12752482 effective words) took 23.9s, 533350 effective words/s\n",
      "2019-05-06 11:34:21,450 : INFO : EPOCH 4 - PROGRESS: at 4.43% examples, 557633 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:34:22,478 : INFO : EPOCH 4 - PROGRESS: at 9.40% examples, 584889 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:34:23,494 : INFO : EPOCH 4 - PROGRESS: at 14.22% examples, 590051 words/s, in_qsize 8, out_qsize 1\n",
      "2019-05-06 11:34:24,491 : INFO : EPOCH 4 - PROGRESS: at 20.00% examples, 624217 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:34:25,502 : INFO : EPOCH 4 - PROGRESS: at 24.83% examples, 621768 words/s, in_qsize 8, out_qsize 0\n",
      "2019-05-06 11:34:26,508 : INFO : EPOCH 4 - PROGRESS: at 28.53% examples, 596238 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:34:27,510 : INFO : EPOCH 4 - PROGRESS: at 32.02% examples, 573126 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:34:28,504 : INFO : EPOCH 4 - PROGRESS: at 35.33% examples, 554323 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:34:29,503 : INFO : EPOCH 4 - PROGRESS: at 38.68% examples, 540400 words/s, in_qsize 6, out_qsize 1\n",
      "2019-05-06 11:34:30,530 : INFO : EPOCH 4 - PROGRESS: at 43.14% examples, 542439 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:34:31,531 : INFO : EPOCH 4 - PROGRESS: at 47.56% examples, 544414 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:34:32,531 : INFO : EPOCH 4 - PROGRESS: at 52.21% examples, 548145 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:34:33,548 : INFO : EPOCH 4 - PROGRESS: at 57.57% examples, 558638 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:34:34,560 : INFO : EPOCH 4 - PROGRESS: at 62.66% examples, 564840 words/s, in_qsize 8, out_qsize 0\n",
      "2019-05-06 11:34:35,573 : INFO : EPOCH 4 - PROGRESS: at 68.32% examples, 574851 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:34:36,571 : INFO : EPOCH 4 - PROGRESS: at 73.96% examples, 583998 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:34:37,588 : INFO : EPOCH 4 - PROGRESS: at 79.63% examples, 591634 words/s, in_qsize 8, out_qsize 0\n",
      "2019-05-06 11:34:38,582 : INFO : EPOCH 4 - PROGRESS: at 84.83% examples, 595658 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:34:39,586 : INFO : EPOCH 4 - PROGRESS: at 90.31% examples, 600984 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:34:40,604 : INFO : EPOCH 4 - PROGRESS: at 95.65% examples, 604286 words/s, in_qsize 8, out_qsize 0\n",
      "2019-05-06 11:34:41,403 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-06 11:34:41,419 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-06 11:34:41,431 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-06 11:34:41,435 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-06 11:34:41,439 : INFO : EPOCH - 4 : training on 17798263 raw words (12749853 effective words) took 21.0s, 607158 effective words/s\n",
      "2019-05-06 11:34:42,466 : INFO : EPOCH 5 - PROGRESS: at 5.19% examples, 663720 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:34:43,472 : INFO : EPOCH 5 - PROGRESS: at 10.87% examples, 686901 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:34:44,474 : INFO : EPOCH 5 - PROGRESS: at 16.72% examples, 701925 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:34:45,483 : INFO : EPOCH 5 - PROGRESS: at 22.76% examples, 717052 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:34:46,491 : INFO : EPOCH 5 - PROGRESS: at 28.82% examples, 725782 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:34:47,504 : INFO : EPOCH 5 - PROGRESS: at 34.88% examples, 731626 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:34:48,499 : INFO : EPOCH 5 - PROGRESS: at 40.86% examples, 736372 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:34:49,501 : INFO : EPOCH 5 - PROGRESS: at 46.72% examples, 737721 words/s, in_qsize 6, out_qsize 1\n",
      "2019-05-06 11:34:50,504 : INFO : EPOCH 5 - PROGRESS: at 52.78% examples, 741479 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:34:51,500 : INFO : EPOCH 5 - PROGRESS: at 58.63% examples, 743019 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:34:52,501 : INFO : EPOCH 5 - PROGRESS: at 64.38% examples, 742300 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:34:53,502 : INFO : EPOCH 5 - PROGRESS: at 70.10% examples, 741128 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:34:54,519 : INFO : EPOCH 5 - PROGRESS: at 75.76% examples, 739373 words/s, in_qsize 8, out_qsize 0\n",
      "2019-05-06 11:34:55,526 : INFO : EPOCH 5 - PROGRESS: at 81.59% examples, 739242 words/s, in_qsize 6, out_qsize 0\n",
      "2019-05-06 11:34:56,537 : INFO : EPOCH 5 - PROGRESS: at 87.48% examples, 739534 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:34:57,547 : INFO : EPOCH 5 - PROGRESS: at 93.12% examples, 738013 words/s, in_qsize 7, out_qsize 0\n",
      "2019-05-06 11:34:58,558 : INFO : EPOCH 5 - PROGRESS: at 99.00% examples, 738287 words/s, in_qsize 8, out_qsize 0\n",
      "2019-05-06 11:34:58,708 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-05-06 11:34:58,720 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-05-06 11:34:58,724 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-05-06 11:34:58,728 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-05-06 11:34:58,732 : INFO : EPOCH - 5 : training on 17798263 raw words (12748582 effective words) took 17.3s, 738362 effective words/s\n",
      "2019-05-06 11:34:58,732 : INFO : training on a 88991315 raw words (63749479 effective words) took 100.5s, 634234 effective words/s\n",
      "2019-05-06 11:34:58,740 : INFO : precomputing L2-norms of word weight vectors\n",
      "2019-05-06 11:34:58,952 : INFO : saving Word2Vec object under 100features_40minwords_10context, separately None\n",
      "2019-05-06 11:34:58,952 : INFO : not storing attribute vectors_norm\n",
      "2019-05-06 11:34:58,957 : INFO : not storing attribute cum_table\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-05-06 11:34:59,259 : INFO : saved 100features_40minwords_10context\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "model = word2vec.Word2Vec(sentences, workers=num_workers, size=num_features, min_count=min_word_count, window=context, sample=downsampling)\n",
    "# init_sims will make the model much more memory-efficient.\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "model_name = \"100features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kitchen'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.doesnt_match(\"man woman child kitchen\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('europe', 0.8573665618896484),\n",
       " ('pakistan', 0.8388616442680359),\n",
       " ('spain', 0.8369898200035095),\n",
       " ('france', 0.8231020569801331),\n",
       " ('china', 0.8115490674972534),\n",
       " ('greece', 0.8082659244537354),\n",
       " ('england', 0.8039284944534302),\n",
       " ('germany', 0.8038352131843567),\n",
       " ('italy', 0.7948799729347229),\n",
       " ('poland', 0.7881669998168945)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar('india')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('princess', 0.7912745475769043)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.most_similar(positive=['woman', 'prince'], negative=['man'])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
